---
layout: single
title: "공정성 (Fairness)"
excerpt: "Types of Bias, Identifying Bias, Evaluating for Bias"
toc: false
toc_sticky: false
categories:
  - machine-learning
---

머신 러닝 모델을 책임감 있게 평가하려면 loss metrics를 계산하는 것 이상의 작업이 필요하다. 모델을 실사용에 적용하기 전에, 학습 데이터를 감사하고, 편향에 대한 예측을 평가하는 것은 매우 중요하다.

학습 데이터에서 나타날 수 있는 다양한 유형의 인간 편향을 알아보자. 이를 식별하고 이에 대한 효과를 평가하는 전략은 큰 도움이 될 것이다.


---
## 편향의 유형 (Type of Bias)

머신 러닝 모델은 본질적으로 객관적이지 않다. 엔지니어는 학습 예제의 데이터 세트를 공급하여 모델을 훈련시킨다. 이 **데이터 세트의 공급 및 큐레이션에 대한 인간의 개입은 모델의 예측을 편향에 취약하게 만들 수 있다.**

모델을 구축할 때, 데이터에 나타날 수 있는 일반적인 인간 편향을 인식하는 것은 중요하다. 이에 대한 영향을 완화하기 위한 사전 조취를 취할 수 있다.

*아래의 편향 목록은 머신 러닝 데이터 세트에서 종종 발견되는 편향 중 일부를 적은 것이다. 위키피디아의 [인식 편향 목록](https://en.wikipedia.org/wiki/List_of_cognitive_biases)에서 더 다양한 유형의 인간 편향을 볼 수 있다. **데이터를 감사할 때, 모델의 예측을 왜곡할 수 있는 모든 잠재적인 편향 소스를 살펴봐야 한다.***


### 보고 편향 (Reporting Bias)

보고 편향은 데이터 세트에 기록된 이벤트, 속성 및 결과의 빈도가 실제 빈도를 정확하게 반영하지 않을 때 발생한다. 이 편향은 **사람들이 일반적으로 당연하다고 생각되는 부분은 그냥 지나치고, 평범하지 않거나 비정상적이거나 특히 기억에 남는 상황을 기록하는 데 집중하는 경향이 있기 때문에 발생할 수 있다.**

> 예시 : 감성 분석 모델은 인기 있는 웹사이트에 대한 사용자 제출 자료를 기반으로 책의 리뷰가 긍정적인지 부정적인지 예측하도록 학습된다. 학습 데이터 세트에 있는 대부분의 리뷰는 극단적인 의견(극정적, 부적정)을 반영한다. 왜냐하면 사람들이 책에 강력하게 응답하지 않으면 책에 대한 리뷰를 제출할 가능성이 적기 때문이다. 결과적으로, 모델은 책을 묘사하기 위해 미묘한 언어를 사용하는 리뷰의 감정을 정확하게 예측할 수 없다.


### 자동화 편향 (Automation Bias)

자동화 편향은 **각각의 오류율에 관계없이 자동화되지 않은 시스템에서 생성된 결과보다 자동화된 시스템에서 생성된 결과를 선호하는 경향이다.**

> 예시 : 사슬톱니바귀 제조업체에서 일하는 소프트웨어 엔지니어들은 공장 감독자가 모델의 정밀도와 재현율이 모두 인간 검사자보다 15% 낮다고 지적할 때까지 톱니 결함을 식별하기 위해 학습한 "획기적인" 모델을 배포하기를 열망했다.


### 선택 편향 (Selection Bias)

선택 편향은 **데이터 세트의 예가 실제 분포를 변영하지 않는 방식으로 선택되면 발생한다.** 선택 편향은 다양한 형태를 취할 수 있다.

- **범위 편향(Coverage bias)** : 데이터가 대표적인 방식으로 선택되지 않았다.

> 예시 : 모델은 제품을 구매한 소비자 표본을 대상으로 실시한 전화 설문조사를 기반으로 신제품의 미래 판매를 예측하도록 학습한다. 그러나 경쟁 제품을 구매하기로 선택한 소비자는 설문 조사를 받지 않았으며, 결과적으로 학습 데이터에 포함되지 않았다.

- **무응답 편향(Non-response bias) 또는 참여 편향(Participation bias)** : 데이터 수집 과정의 참여 격차로 인해 데이터가 대표성이 없게 된다.

> 예시 : 모델은 제품을 구매한 소비자 표본과 경쟁 제품을 구매한 소비자 표본을 대상으로 실시한 전화 설문조사를 기반으로 신제품에 대한 미래 판매를 예측하도록 학슴된다. 경쟁 제품을 구매한 소비자는 설문 조사를 거부할 가능성이 80% 더 높았으며, 샘플에서 데이터가 과소 표현되었다.

- **샘플링 편향 (Sampling bias)** : 데이터 수집 중에는 적절한 무작위화가 사용되지 않는다.

> 예시 : 모델은 제품을 구매한 소비자 표본과 경쟁 제품을 구매한 소비자 표본을 대상으로 실시한 전화 설문조사를 기반으로 신제품의 미래 판매를 예측하도록 훈련됩니다. 조사자는 소비자를 무작위로 타켓팅하는 대신 이메일에 응답한 처음 200명의 소비자를 선택했다.


### 집단 귀인 편향 (Group Attribution Bias)

집단 귀인 편향은 **개인의 진실을 그들이 속한 전체 집단으로 일반화하는 성향이다.** 이 편향의 두 가지 중요 징후는 다음과 같다.

- **그룹 내 편향(In-group bias)** : 그 사람이 속한 그룹의 구성원 또는 그 사람이 공유하는 특성에 대한 선호이다.

> 예시 : 소프트웨어 개발자를 위한 이력서 심사 모델을 학습하는 두 명의 엔지니어는 둘 다 같은 컴퓨터 과학 학원에 다녔던 지원자가 그 역할에 더 적합하다고 믿는 경향이 있다.

- **그룹 외 동질성 편향(Out-group homogeneity bias)** : 자신이 속하지 않은 집단의 구성원 개개인을 고정관념에 빠뜨리거나 그들의 특성을 더 획일적으로 보는 성향.

> 예시 : 소프트웨어 개발자를 위한 이력서 심사 모델을 학습하는 두 명의 엔지니어는 컴퓨터 과학 학원에 다니지 않은 모든 지원자가 해당 역할에 대한 충분한 전문 지식을 갖고 있지 않다고 믿는 경향이 있다.


### 내재적 편향 (Implicit Bias)

내재적 편향은 **일반적으로 적용되지 않는 자신의 정신적 모델과 개인적 경험을 기반으로 추정할 때 발생한다.**

> 제스처 인식 모델을 학습시키는 엔지니어는 사람이 "아니오"라는 단어를 전달하고 있음을 나타내는 기능으로 머리 흔드는 것을 사용한다. 그러나 세계 중 일부 지역에서는 머리를 흔드는 것이 실제로 "예"를 의미한다.

내재적 편향의 일반적인 형태는 **확증 편향(confirmation bias)**으로, 모델 구축자는 기존 신념과 가설을 긍정하는 방식으로 데이터를 무의식적으로 처리한다. 어떤 경우에는, 모델 구축자가 원래 가설과 일치하는 결과를 생성할 때까지 실제로 모델을 계속 학습시킨다. 이를 **실험자 편향(experimenter's bias)**라고 한다.

> 예시 : 엔지니어는 다양한 특성(키, 체중, 품종, 환경)을 기반으로 개의 공격성을 예측하는 모델을 구축하고 있다. 이 엔지니어는 어렸을 때 지나치게 활동적인 토이 푸들과 불쾌한 만남을 가졌고, 그 이후로 그 품종을 공격적이라고 생각했다. 학습된 모델이 대부분의 토이 푸들이 상대적으로 유순하다고 예측했을 때, 엔지니어는 더 작은 푸들이 더 폭력적이라는 결과가 나올 때까지 모델을 여러 번 다시 학습시켰다.


---
## 편향 식별 (Identifying Bias)

모델에서 데이터를 가장 잘 표현하는 방법을 결정하기 위해, **공정성 문제를 염두에 두고 편향의 잠재적 원인에 대해 사전에 감사하는 것도 중요하다.**

편향은 어디에 숨어있을 수 있는가? 다음은 데이터 세트에서 주의해야 할 세 가지 위험 신호이다.


### 누락된 특성 값 (Missing Feature Value)

데이터 세트의 많은 데이터에 대해 하나 이상의 결측값을 가지고 있는 특성이 있는 경우, 그 특성이 과소 표현된다는 지표가 될 수 있다.

예를 들어, 아래 표는 `pandas DataFrame`으로 불러 `describe` 메소드를 사용해 생성된 'California Housing' 데이터 세트의 numerical 특성에 대한 통계 요약이다. 'count' 열을 보면 총 데이터 수는 17,000개 이며, 누락된 값이 없음을 알 수 있다.

<center><img src="{{site.baseurl}}/assets/images/fairness1.png" /></center><br>

그렇다면, 'population', 'households', 'median_income' 이 세 특성의 'count'가 3000개라고 가정하자. 즉, 이 세 특성은 각각 14,000개의 결측값이 있다는 것이다.

<center><img src="{{site.baseurl}}/assets/images/fairness2.png" /></center><br>

이 14,000개의 누락된 값으로 인해 가구의 평균 소득과 주택의 중간 가격을 정밀하게 연관시키는 것이 훨씬 더 어려워진다. 이 데이터에 대한 **모델을 학습시키기 전에 이러한 누락된 값의 원인을 조사하여 소득 및 인구 데이터 누락에 대한 잠재적인 편향이 없는지 확인하는 것이 좋다.**


### 예상치 못한 특성값 (Unexpected Feature Value)

데이터를 탐색할 때, 특히 특이하거나 흔치 않은 특성값이 포함된 데이터도 찾아야 한다. 이러한 예기치 못한 특성값은 데이터 수집 중에 발생한 문제거나, 편향을 유발할 수 있는 기타 부정확성일 수 있다.

'California Housing' 데이터 세트에서 발췌한 다음 예를 보자.

<center><img src="{{site.baseurl}}/assets/images/fairness3.png" /></center><br>

위의 표에서 예상치 못한 값을 정확하게 찾아낼 수 있는가?

<center><img src="{{site.baseurl}}/assets/images/fairness4.png" /></center><br>

4행의 위도(longitude)와 경도(latitude)의 좌표(-103, 43.8)는 미국 캘리포니아 주에 속하지 않는다. 이 좌표는 사우스다코타 주의 '러시모어 산 국립기념관'의 좌표이다.


### 데이터 왜곡 (Data Skew)

특정 그룹이나 특성이 실제에 비해 과소 표현되거나 과대 표현될 수 있는 **데이터의 모든 종류의 왜곡은 모델에 편향을 유발할 수 있다.**

모델 검증까지 마쳤다면, 'California Housing' 데이터 세트를 training set와 validation set로 분할하기 전에 랜덤 샘플링에 실패하여 데이터 왜곡이 현저하게 발생했을음 발견한 것을 기억할 것이다. 다음 그림은 캘리포니아 북서부 지역의 데이터를 중점적으로 나타내는 데이터를 시각화한 것이다.

<center><img src="{{site.baseurl}}/assets/images/fairness5.png" /></center><br>

위의 그림은 데이터의 위도와 경도를 캘리포니아 지도 위에 오버레이하여 시각화한 것이다. 각 점은 주택을 나타내며 파란색에서 빨간색으로 갈수록 주택가격이 상승한다.

**이처럼 대표성이 없는 샘플을 사용하여 주 전체의 캘리포니아 주택 가격을 예측하는 모델을 학습하는 경우, 캘리포니아 남부 지역의 주택 데이터가 부족하여 문제가 될 수 있다.**


---
## 편향 평가 (Evaluating for Bias)

**모델을 평가할 때, 전체 test set 또는 validation set에 따라 계산된 매트릭스는 항상 모델이 얼마나 공정한지를 정확하게 보여주지는 않는다.** 따라서 모델에 편향이 있는지 평가해 보는 것도 필요하다.

1,000명의 환자의 의료 기록의 validation set에 대해 평가되는, 종양의 존재를 예측하기 위해 개발된 새로운 모델이 개발되었다고 가정하자. 500개의 기록은 여성 환자이고 나머지 500개는 남성 환자의 기록이다. 다음의 혼돈 행렬(confusion matrix)는 모든 데이터에 대한 결과이다.

<center><img src="{{site.baseurl}}/assets/images/fairness6.png" /></center><br>

80%의 정밀도와 72.7%의 재현율로 양호한 모델로 보인다. 그러나 각 환자 세트에 대해 별도로 결과를 계산하면 어떨까? 위의 결과를 다시 두 개의 confusion matrix로 나눠보겠다. 하나는 여성 환자, 다른 하나는 남성 환자이다.

<center><img src="{{site.baseurl}}/assets/images/fairness7.png" /></center><br>

**여성 환자와 남성 환자에 대한 메트릭스를 별도로 계산할 때, 각 그룹에 대한 모델의 성능이 큰 차이를 보이고 있을을 알 수 있다.**

- 여성 환자 :
  - 실제로 종양을 가지고 있는 11명의 여성 환자 중, 모델은 10명의 환자에 대해 정확하게 예측했다 (재현율 : 90.9%). 즉, 모델은 여성 데이터의 9.1%를 놓친다.
  - 유사하게, 모델이 여성 환자의 종양에 대해 positive를 반환하면 11건 중 10건은 정확하다는 뜻이다 (정밀도 : 90.9%). 즉, 모델은 여성 데이터의 9.1%를 잘못 예측한다.

- 남성 환자 :
  - 실제로 종양의 가지고 있는 11명의 남성 환자 중, 모델은 6명의 환자에 대해 정확하게 예측했다 (재현율 : 54.5%). 이는 모델이 남성 데이터의 54.5%를 놓친다는 것의 의미한다.
  - 그리고 모델이 남성 환자의 종양에 대해 positive를 반환할 때, 9건 중 6건에서만 정확하다 (정밀도 : 66.7%). 즉, 모델은 남성 데이터의 33.3%를 잘못 예측한다.

이제 모델의 예측에 내재된 편향과, 모델이 일반 대중에 의료용으로 출시될 경우 각 하위 그룹에 대한 위험에 대해 잘 이해할 수 있을 것이다.

<br>
<br>
<br>

---
※ 참고

[Google Developer - Types of Bias](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias)<br>
[Google Developer - Identifying Bias](https://developers.google.com/machine-learning/crash-course/fairness/identifying-bias)<br>
[Google Developer - Evaluating for Bias](https://developers.google.com/machine-learning/crash-course/fairness/evaluating-for-bias)<br>